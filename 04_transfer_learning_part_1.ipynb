{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-27 00:22:29--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.115.207, 142.250.113.207, 142.250.114.207, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.115.207|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 168546183 (161M) [application/zip]\n",
      "Saving to: ‘10_food_classes_10_percent.zip’\n",
      "\n",
      "10_food_classes_10_ 100%[===================>] 160.74M  40.5MB/s    in 4.1s    \n",
      "\n",
      "2024-06-27 00:22:34 (39.3 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_ref = zipfile.ZipFile('10_food_classes_10_percent.zip')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 directories and 0 images in \"10_food_classes_10_percent\".\n",
      "There are 10 directories and 0 images in \"10_food_classes_10_percent/test\".\n",
      "There are 0 directories and 250 images in \"10_food_classes_10_percent/test/grilled_salmon\".\n",
      "There are 0 directories and 250 images in \"10_food_classes_10_percent/test/chicken_curry\".\n",
      "There are 0 directories and 250 images in \"10_food_classes_10_percent/test/hamburger\".\n",
      "There are 0 directories and 250 images in \"10_food_classes_10_percent/test/ice_cream\".\n",
      "There are 0 directories and 250 images in \"10_food_classes_10_percent/test/ramen\".\n",
      "There are 0 directories and 250 images in \"10_food_classes_10_percent/test/sushi\".\n",
      "There are 0 directories and 250 images in \"10_food_classes_10_percent/test/chicken_wings\".\n",
      "There are 0 directories and 250 images in \"10_food_classes_10_percent/test/pizza\".\n",
      "There are 0 directories and 250 images in \"10_food_classes_10_percent/test/steak\".\n",
      "There are 0 directories and 250 images in \"10_food_classes_10_percent/test/fried_rice\".\n",
      "There are 10 directories and 0 images in \"10_food_classes_10_percent/train\".\n",
      "There are 0 directories and 75 images in \"10_food_classes_10_percent/train/grilled_salmon\".\n",
      "There are 0 directories and 75 images in \"10_food_classes_10_percent/train/chicken_curry\".\n",
      "There are 0 directories and 75 images in \"10_food_classes_10_percent/train/hamburger\".\n",
      "There are 0 directories and 75 images in \"10_food_classes_10_percent/train/ice_cream\".\n",
      "There are 0 directories and 75 images in \"10_food_classes_10_percent/train/ramen\".\n",
      "There are 0 directories and 75 images in \"10_food_classes_10_percent/train/sushi\".\n",
      "There are 0 directories and 75 images in \"10_food_classes_10_percent/train/chicken_wings\".\n",
      "There are 0 directories and 75 images in \"10_food_classes_10_percent/train/pizza\".\n",
      "There are 0 directories and 75 images in \"10_food_classes_10_percent/train/steak\".\n",
      "There are 0 directories and 75 images in \"10_food_classes_10_percent/train/fried_rice\".\n",
      "['chicken_curry' 'chicken_wings' 'fried_rice' 'grilled_salmon' 'hamburger'\n",
      " 'ice_cream' 'pizza' 'ramen' 'steak' 'sushi']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk('10_food_classes_10_percent'):\n",
    "    print(f'There are {len(dirnames)} directories and {len(filenames)} images in \"{dirpath}\".')\n",
    "\n",
    "data_dir = pathlib.Path('10_food_classes_10_percent/train')\n",
    "class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))\n",
    "\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 750 images belonging to 10 classes.\n",
      "Found 2500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMAGE_SHAPE = (224, 244)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "\n",
    "train_dir = '10_food_classes_10_percent/train/'\n",
    "test_dir = '10_food_classes_10_percent/test/'\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(train_dir,\n",
    "                                               target_size=IMAGE_SHAPE,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               class_mode='categorical')\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(test_dir,\n",
    "                                             target_size=IMAGE_SHAPE,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_tensorboard_callback(dir_name, experiment_name):\n",
    "    log_dir = dir_name + '/' + experiment_name + '/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    print(f'Saving TensorBoard log files to: {log_dir}')\n",
    "    return tensorboard_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./.local/lib/python3.10/site-packages (from tensorflow_hub) (1.26.0)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in ./.local/lib/python3.10/site-packages (from tensorflow_hub) (4.23.4)\n",
      "Collecting tf-keras>=2.14.1\n",
      "  Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow<2.17,>=2.16\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in ./.local/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.local/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (1.6.3)\n",
      "Collecting ml-dtypes~=0.3.1\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./.local/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (1.14.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.local/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (16.0.6)\n",
      "Collecting tensorboard<2.17,>=2.16\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras>=3.0.0\n",
      "  Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in ./.local/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.local/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (1.59.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (59.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.local/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (4.8.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./.local/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (23.5.26)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.local/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.local/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (0.5.4)\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./.local/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (3.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.local/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (2.0.0)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (23.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.local/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (2.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.local/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (0.34.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (0.37.1)\n",
      "Collecting rich\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 KB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting optree\n",
      "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 KB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting namex\n",
      "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (3.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (3.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (3.5.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (3.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (0.7.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (2.1.3)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.local/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow_hub) (2.16.1)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, optree, ml-dtypes, mdurl, tensorboard, markdown-it-py, rich, keras, tensorflow, tf-keras, tensorflow_hub\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.2.0\n",
      "    Uninstalling ml-dtypes-0.2.0:\n",
      "      Successfully uninstalled ml-dtypes-0.2.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.15.1\n",
      "    Uninstalling tensorboard-2.15.1:\n",
      "      Successfully uninstalled tensorboard-2.15.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.15.0.post1\n",
      "    Uninstalling tensorflow-2.15.0.post1:\n",
      "      Successfully uninstalled tensorflow-2.15.0.post1\n",
      "Successfully installed keras-3.4.1 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 rich-13.7.1 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow_hub-0.16.1 tf-keras-2.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "def create_model(model_url, num_classes=10):\n",
    "    feature_extractor_layer = hub.KerasLayer(model_url,\n",
    "                                             trainable=False,\n",
    "                                             name='feature_extraction_layer',\n",
    "                                             input_shape=IMAGE_SHAPE+(3,))\n",
    "    model = tf.keras.Sequential([\n",
    "        feature_extractor_layer,\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', name='output_layer')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_url = \"https://www.kaggle.com/models/tensorflow/efficientnet/TensorFlow2/b0-feature-vector/1\"\n",
    "\n",
    "efficientnet_model = create_model(efficientnet_url,\n",
    "                                  )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
